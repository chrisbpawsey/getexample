#!/bin/bash --login
# This is the README file is an executable script.
# To run type: ./README
#
# Example OBJECTIVE: to demonstrate gromacs example on Paragon
# with the GNU compilers and OpenMPI.
# 
# In this example Gromacs is using the MPI executable that runs 
# on 2 nodes or more with up to 16 MPI tasks per node.
# To run this code, we need to specify the total number of MPI tasks
# and the number of MPI tasks per node.
# We also need to define the correct module environment
# to be able to run this example.
# We need to load the module gromacs/2018.3 as well as any other
# required environment modules.  Thie MUST be done prior to 
# submit any job to the queue.  This is due to a flaw in the implementation
# and configuration of the MULTI-CLUSTER at the HARTREE Centre.

# Notes on LSF directives
# 
# Here we specify to LSF that we want to limit the number of tasks per node
# -R "span[ptile="NUMBER OF TASKS"]"
# The NUMBER OF TASKS can be between 1 and 16 on Paragon.
# So two nodes with 16 tasks each would need
# #BSUB -n 32
# #BSUB -R "span[ptile=16]"

# 4 nodes with 8 task per node would look like 
# #BSUB -n 32
# #BSUB -R "span[ptile=8]"

#
# Load environment using module command
module list
module load use.paragon
module load gcc6/6.4.0 openmpi/3.1.0 gromacs/2018.3 

# Create a temp-storage directory and copy data and/or executables
# Note! copying executables is not needed if using environment modules

if [ ! -d $PG/data_exe ]; then
   mkdir -p $PG/data_exe
fi



# Download data necessary for this gromacs example
# wget https://files.rcsb.org/download/1AKI.pdb.gz
# note the 1AKI.pdb file must be edited first by removing all the 
# lines with HOH (water) in it. The one included in this getexample
# has been modified from the original.

# Download the *.mdp files needed.
if [ ! -f GROMACS_TestCaseA.tar.gz ]; then
 wget http://www.prace-ri.eu/UEABS/GROMACS/1.2/GROMACS_TestCaseA.tar.gz 
fi

cp GROMACS_TestCaseA.tar.gz $PG/data_exe/.

export SUBMIT_DIR=`pwd`

# To submit the job to paragon

jobid=$(bsub <  gromacs_praceA.lsf)
jobid=`echo $jobid | awk '{print $2}' | sed 's/<//g' | sed 's/>//g'`

echo " "
echo "The lsf command returns what the jobid is for this job."
echo "To check the status of your job, use the lsf command:"
echo "bjobs -a -u $USER"
echo " "
echo "Your job will be run in /gpfs/paragon/../run_gromacs_praceA/${jobid}."
echo " "
echo "Your results will be saved in ${MYCDS}/gromacs_praceA_results/${jobid}"
echo "and the scratch directory will then be deleted."
echo " "
echo "To check the results change to your jobid directory, type:"
echo "cd ${MYCDS}/gromacs_praceA_results/${jobid}"
echo " " 
echo "To view the results, use the cat command and type:"
echo "cat praceA_gromacs.log"
echo " "
echo " "

# lets script run in background until job finishes
while [[ ! -f $PG/cpt$jobid ]]; do
  sleep 2
done

# Declare a unique directory to store results
RESULTS=$HCCDS/gromacs_praceA_results
#####################################################
#  Make the unique directory on CDS in the project directory for the results of this job
if [ ! -d $RESULTS ]; then
  mkdir -p $RESULTS
fi
echo the results directory is $RESULTS

# move results from scratch back to CDS
mv ${PG}/run_gromacs_praceA/$jobid $RESULTS

####################################################
# Clean up $SCRATCH directory on parallel file system on cluster

#rm -rf ${PG}/run_gromacs_praceA/$jobid
#rm ${PG}/cpt${jobid}


