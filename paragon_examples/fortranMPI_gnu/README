#!/bin/bash --login
# This is the README file is an executable script.
# To run type: ./README
#
# Example OBjECTIVE: to demostrate hello_mpi.f90 example
# on Zeus. 
# This code is an MPI example that requests 2 nodes with 16
# MPI tasks running on each node.
# This example is taken from:
# https://people.sc.fsu.edu/~jburkardt/f_src/hello_mpi/hello_mpi.html
# cp the hello_mpi.f90 source code
cp ../../src/hello_mpi.f90 .
# patch hello_mpi.f90
sed -i "s/use mpi/include \"mpif.h\"/g" hello_mpi.f90
 
# To run this code load the necessary modules.
# and specify the total number of MPI tasks.
# This information is located under hello_mpi_gnu.slurm
# You can edit the SLURM as: emacs hello_mpi_gnu.slurm &

# LSF directives
# 
# To compile the code with GNU compiler, 
# we need to load the gcc module as shown below:
module load gcc/7.4.0 openmpi/3.1.3

# We also specify to srun that 32 MPI tasks are required. (-n 32)
# with 2 nodes (-N 2) given that 
# there are 16 MPI tasks per node.
# For the correct operation, MPI also needs to be specified to srun
# via the option "--mpi=pmi2"
# Therefore, the srun command looks like:
# srun --mpi=pmi2 -n 32 -N 2 ./$EXECUTABLE >> ${OUTPUT}
 
# To compile the hello_mpi.f90 code
mpif90 -O3 -mcpu=power8  hello_mpi.f90 -o hello_mpi_gnu

if [ ! -d ${PG}/data_exe ]; then
  mkdir -p ${PG}/data_exe  
fi

# copy the executable to the data_exe directory on paragon scratch.
cp hello_mpi_gnu $PG/data_exe/.
# sanity check that the executable is there
ls -al $PG/data_exe

# To submit the job to paragon
id=$(bsub < hello_mpi_gnu.lsf)
jobid=`echo $id | awk '{print $2}' | sed 's/<//g' | sed 's/>//g'`

echo "The sbatch command returns what the jobid is for this job."
echo "To check the status of your job, use the slurm command:"
echo "bjobs -u $USER"
echo " "
echo "Your job will be run in $PG/run_fortranMPI_gnu/${jobid}."
echo " "
echo "Your results will be saved in ${PG}/fortranMPI_gnu_results_paragon/${jobid}"
echo "and the scratch directory will then be deleted."
echo " "
echo "To check the results change to your jobid directory, type:"
echo "cd ${PG}/fortranMPI_gnu_results_paragon/${jobid}"
echo " " 
echo "To view the results, use the cat command and type:"
echo "cat fortranMPI_gnu.log"
echo " "

#  lets script run in background to until job finishes
while [[ ! -f $PG/cpt$jobid ]]; do
  sleep 2
done

#  Declare unique directory to store results
RESULTS=$MYCDS/fortranMPI_gnu_results_paragon
###############################################
# Makes the unique directory in CDS directory for the results of this job
if [ ! -d $RESULTS ]; then 
    mkdir -p $RESULTS 
fi
echo the results directory is $RESULTS

# move results from scratch back to CDS
mv  ${PG}/run_fortranMPI_gnu/$jobid  $RESULTS

###########################
# Clean up $SCRATCH 

rm -rf ${PG}/run_fortranMPI_gnu/$jobid
rm ${PG}/cpt$jobid

